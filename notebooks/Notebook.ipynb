{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "import time\n",
        "import json\n",
        "from tqdm import tqdm\n",
        "from transformers import pipeline, AutoModelForSequenceClassification, AutoTokenizer\n",
        "from sentence_transformers import SentenceTransformer\n",
        "import spacy\n",
        "import gc\n",
        "import hnswlib\n",
        "\n",
        "class ProductRecommender:\n",
        "    def __init__(self, dataframe_path, max_dataset_size=200000, chunk_size=2000, top_n=10, absa_chunk_size=400, absa_batch_size=16):\n",
        "        os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"expandable_segments:True\"\n",
        "\n",
        "        if torch.cuda.is_available():\n",
        "            self.overall_device_str = 'cuda'\n",
        "            self.overall_device_idx = 0\n",
        "            print(f\"📦 **Overall Device**: GPU (cuda:{self.overall_device_idx})\")\n",
        "        else:\n",
        "            self.overall_device_str = 'cpu'\n",
        "            self.overall_device_idx = -1\n",
        "            print(f\"📦 **Overall Device**: CPU\")\n",
        "\n",
        "        self.chunk_size = chunk_size\n",
        "        self.top_n = top_n\n",
        "        self.max_dataset_size = max_dataset_size\n",
        "        self.absa_chunk_size = absa_chunk_size\n",
        "        self.absa_batch_size = absa_batch_size\n",
        "\n",
        "        print(f\"⚙️ **ABSA Settings**: chunk_size={self.absa_chunk_size}, batch_size={self.absa_batch_size}\")\n",
        "\n",
        "        self.df_original = pd.read_csv(dataframe_path)\n",
        "        self.df = None\n",
        "\n",
        "        # Load ABSA model\n",
        "        absa_model_name = \"yangheng/deberta-v3-base-absa-v1.1\"\n",
        "        self.absa_pipeline_device_idx = self.overall_device_idx\n",
        "        try:\n",
        "            absa_tokenizer = AutoTokenizer.from_pretrained(absa_model_name)\n",
        "            absa_model = AutoModelForSequenceClassification.from_pretrained(\n",
        "                absa_model_name,\n",
        "                device_map=\"auto\"\n",
        "            )\n",
        "            print(f\"✅ **ABSA Model loaded with device_map='auto'**.\")\n",
        "        except Exception as e:\n",
        "            print(f\"⚠️ **Failed to load ABSA model with device_map='auto'**: {e}. Falling back.\")\n",
        "            absa_tokenizer = AutoTokenizer.from_pretrained(absa_model_name)\n",
        "            absa_model = AutoModelForSequenceClassification.from_pretrained(absa_model_name)\n",
        "            absa_model.to(self.overall_device_str)\n",
        "            print(f\"✅ **ABSA Model loaded on {self.overall_device_str}**.\")\n",
        "\n",
        "        if hasattr(absa_model, 'hf_device_map') and absa_model.hf_device_map is not None:\n",
        "            self.absa_pipe = pipeline(\"text-classification\", model=absa_model, tokenizer=absa_tokenizer)\n",
        "            print(\"✅ **ABSA pipeline initialized (managed by Accelerate)**.\")\n",
        "        else:\n",
        "            self.absa_pipe = pipeline(\"text-classification\", model=absa_model, tokenizer=absa_tokenizer, device=self.absa_pipeline_device_idx)\n",
        "            print(f\"✅ **ABSA pipeline initialized on {'GPU' if self.absa_pipeline_device_idx != -1 else 'CPU'}**.\")\n",
        "\n",
        "        self.sbert = SentenceTransformer('all-MiniLM-L6-v2', device=self.overall_device_str)\n",
        "        print(f\"✅ **SBERT Model loaded on {self.overall_device_str}**.\")\n",
        "\n",
        "        # Load spaCy for dynamic aspect extraction\n",
        "        self.nlp = spacy.load(\"en_core_web_sm\", disable=[\"ner\", \"lemmatizer\"])  # Lightweight model\n",
        "\n",
        "        # File paths\n",
        "        self.annotated_path = \"/content/drive/MyDrive/finalyearResearch/Second_fixed_image_urls.csv\"\n",
        "        self.incremental_annotated_path = \"/content/drive/MyDrive/finalyearResearch/annoated/absa_annotated_data_incremental.csv\"\n",
        "        self.itemname_emb_path = \"/content/drive/MyDrive/finalyearResearch/itemname_embeddings.npy\"\n",
        "        self.combo_text_emb_path = \"/content/drive/MyDrive/finalyearResearch/combo_text_embeddings.npy\"\n",
        "        self.enriched_item_descriptions_emb_path = \"/content/drive/MyDrive/finalyearResearch/enriched_item_descriptions_embeddings.npy\"\n",
        "        self.hnsw_index_path = \"/content/drive/MyDrive/finalyearResearch/hnsw_index.bin\"\n",
        "\n",
        "        # Ensure directory exists\n",
        "        os.makedirs(os.path.dirname(self.incremental_annotated_path), exist_ok=True)\n",
        "        print(f\"✅ **Ensured directory exists for incremental file: {os.path.dirname(self.incremental_annotated_path)}**\")\n",
        "\n",
        "        self.item_unique_id_to_name_map = None\n",
        "        self.item_unique_id_to_enriched_text_map = None\n",
        "        self.hnsw_index = None\n",
        "\n",
        "        self.df = self._prepare_data()\n",
        "\n",
        "    def _extract_dynamic_aspects(self, review):\n",
        "        \"\"\"\n",
        "        Extract aspects dynamically from a review using spaCy noun phrase detection.\n",
        "        \"\"\"\n",
        "        doc = self.nlp(review)\n",
        "        aspects = []\n",
        "        for chunk in doc.noun_chunks:\n",
        "            aspect = chunk.text.lower().strip()\n",
        "            if len(aspect) > 2 and aspect not in {'it', 'this', 'that', 'thing', 'product', 'item'}:\n",
        "                aspects.append(aspect)\n",
        "        return list(set(aspects))[:3]\n",
        "\n",
        "    def _extract_multi_aspects(self, reviews):\n",
        "        \"\"\"\n",
        "        Extract dynamic aspects and their sentiments from a list of reviews.\n",
        "        \"\"\"\n",
        "        results = []\n",
        "        for review in reviews:\n",
        "            review_results = {}\n",
        "            aspects = self._extract_dynamic_aspects(review)\n",
        "            if not aspects:\n",
        "                aspects = ['general']\n",
        "            for aspect in aspects:\n",
        "                input_text = f\"[CLS] {review} [SEP] {aspect} [SEP]\"\n",
        "                try:\n",
        "                    result = self.absa_pipe(input_text, batch_size=1)[0]\n",
        "                    sentiment = result['label'].capitalize()\n",
        "                    score = result['score']\n",
        "                    if score > 0.6:\n",
        "                        review_results[aspect] = {'sentiment': sentiment, 'confidence': score}\n",
        "                    else:\n",
        "                        review_results[aspect] = {'sentiment': 'Neutral', 'confidence': 0.0}\n",
        "                except Exception as e:\n",
        "                    print(f\"⚠️ Error processing aspect '{aspect}' for review: {e}\")\n",
        "                    review_results[aspect] = {'sentiment': 'Neutral', 'confidence': 0.0}\n",
        "            results.append(review_results)\n",
        "        return results\n",
        "\n",
        "    def _prepare_data(self):\n",
        "        \"\"\"\n",
        "        Load pre-annotated data and embeddings, cleaning aspects_sentiments column.\n",
        "        \"\"\"\n",
        "        if os.path.exists(self.annotated_path):\n",
        "            print(f\"📂 **Final annotated file found at '{self.annotated_path}'. Loading it.**\")\n",
        "            try:\n",
        "                df = pd.read_csv(self.annotated_path)\n",
        "                # Clean aspects_sentiments column\n",
        "                df['aspects_sentiments'] = df['aspects_sentiments'].apply(\n",
        "                    lambda x: '{}' if pd.isna(x) or not isinstance(x, str) else x\n",
        "                )\n",
        "                if 'aspects_sentiments' in df.columns:\n",
        "                    print(\"✅ **Data appears complete with dynamic ABSA annotations.**\")\n",
        "                    self._save_embeddings(df)\n",
        "                    return df\n",
        "                else:\n",
        "                    print(\"⚠️ **Final file missing dynamic ABSA annotations. Reprocessing.**\")\n",
        "            except Exception as e:\n",
        "                print(f\"⚠️ **Error loading final annotated file: {e}. Reprocessing.**\")\n",
        "\n",
        "        df = self.df_original.sample(n=min(self.max_dataset_size, len(self.df_original)), random_state=42).copy()\n",
        "        df = df[df['reviewText'].str.len() > 15].reset_index(drop=True)\n",
        "        df['description'] = df['description'].fillna('').astype(str).apply(lambda x: x.replace(\"['\", \"\").replace(\"']\", \"\").replace(\"', '\", \", \"))\n",
        "        df['feature'] = df['feature'].fillna('').astype(str).apply(lambda x: x.replace(\"['\", \"\").replace(\"']\", \"\").replace(\"', '\", \", \"))\n",
        "        df['item_unique_id'] = df['itemName'].astype(str) + df['category'].astype(str) + df['description'].astype(str) + df['feature'].astype(str)\n",
        "\n",
        "        processed_df = None\n",
        "        df_to_process = df.copy()\n",
        "\n",
        "        if os.path.exists(self.incremental_annotated_path):\n",
        "            print(f\"📂 **Found incremental data file: {self.incremental_annotated_path}. Checking...**\")\n",
        "            try:\n",
        "                processed_df = pd.read_csv(self.incremental_annotated_path)\n",
        "                processed_df['description'] = processed_df['description'].fillna('').astype(str).apply(lambda x: x.replace(\"['\", \"\").replace(\"']\", \"\").replace(\"', '\", \", \"))\n",
        "                processed_df['feature'] = processed_df['feature'].fillna('').astype(str).apply(lambda x: x.replace(\"['\", \"\").replace(\"']\", \"\").replace(\"', '\", \", \"))\n",
        "                processed_df['item_unique_id'] = processed_df['itemName'].astype(str) + processed_df['category'].astype(str) + processed_df['description'].astype(str) + processed_df['feature'].astype(str)\n",
        "                # Clean aspects_sentiments in incremental data\n",
        "                processed_df['aspects_sentiments'] = processed_df['aspects_sentiments'].apply(\n",
        "                    lambda x: '{}' if pd.isna(x) or not isinstance(x, str) else x\n",
        "                )\n",
        "\n",
        "                processed_ids = set(processed_df['item_unique_id'])\n",
        "                df_to_process = df[~df['item_unique_id'].isin(processed_ids)].copy()\n",
        "                print(f\"✅ **Loaded {len(processed_df)} previously annotated rows.**\")\n",
        "                print(f\"🧠 **{len(df_to_process)} rows remaining to be annotated.**\")\n",
        "            except Exception as e:\n",
        "                print(f\"⚠️ **Error loading incremental file: {e}. Starting from scratch.**\")\n",
        "                processed_df = None\n",
        "                df_to_process = df.copy()\n",
        "                if os.path.exists(self.incremental_annotated_path):\n",
        "                    try:\n",
        "                        os.remove(self.incremental_annotated_path)\n",
        "                        print(f\"✅ **Removed corrupted incremental file: {self.incremental_annotated_path}**\")\n",
        "                    except Exception as e:\n",
        "                        print(f\"⚠️ **Failed to remove incremental file: {e}**\")\n",
        "\n",
        "        if df_to_process.empty:\n",
        "            print(\"✅ **No new rows to process. Finalizing data.**\")\n",
        "            df = processed_df.copy()\n",
        "        else:\n",
        "            print(f\"🧠 **Running Dynamic ABSA on {len(df_to_process)} rows...**\")\n",
        "            start_time = time.time()\n",
        "\n",
        "            write_mode = 'a' if processed_df is not None and not processed_df.empty else 'w'\n",
        "            header = False if processed_df is not None and not processed_df.empty else True\n",
        "\n",
        "            pbar = tqdm(range(0, len(df_to_process), self.absa_chunk_size), desc=\"Performing Dynamic ABSA\")\n",
        "            for i in pbar:\n",
        "                chunk_df = df_to_process.iloc[i:i + self.absa_chunk_size].copy()\n",
        "                reviews_list = chunk_df['reviewText'].tolist()\n",
        "\n",
        "                try:\n",
        "                    results = self._extract_multi_aspects(reviews_list)\n",
        "                    chunk_df['aspects_sentiments'] = [json.dumps(res) for res in results]\n",
        "                    try:\n",
        "                        chunk_df.to_csv(self.incremental_annotated_path, mode=write_mode, header=header, index=False)\n",
        "                        with open(self.incremental_annotated_path, 'a') as f:\n",
        "                            f.flush()\n",
        "                            os.fsync(f.fileno())\n",
        "                        print(f\"✅ **Saved {len(chunk_df)} rows to incremental file (chunk {i//self.absa_chunk_size + 1})**\")\n",
        "                        write_mode = 'a'\n",
        "                        header = False\n",
        "                    except Exception as e:\n",
        "                        print(f\"❌ **Error saving chunk to incremental file: {e}**\")\n",
        "                        raise\n",
        "                    del chunk_df, reviews_list, results\n",
        "                    if self.overall_device_idx != -1:\n",
        "                        torch.cuda.empty_cache()\n",
        "                    gc.collect()\n",
        "                except RuntimeError as e:\n",
        "                    if \"CUDA out of memory\" in str(e):\n",
        "                        print(f\"\\nFATAL ERROR: CUDA Out of Memory. GPU memory: {torch.cuda.memory_summary()}\")\n",
        "                        print(\"ACTION: Reduce absa_batch_size (e.g., to 1) and absa_chunk_size, then restart.\")\n",
        "                        raise\n",
        "                    else:\n",
        "                        print(f\"❌ **Error processing chunk: {e}**\")\n",
        "                        raise\n",
        "\n",
        "            print(f\"✅ **Dynamic ABSA annotation complete in {time.time() - start_time:.2f} seconds.**\")\n",
        "\n",
        "            print(\"🔄 **Combining results...**\")\n",
        "            try:\n",
        "                df = pd.read_csv(self.incremental_annotated_path)\n",
        "                # Clean aspects_sentiments in combined data\n",
        "                df['aspects_sentiments'] = df['aspects_sentiments'].apply(\n",
        "                    lambda x: '{}' if pd.isna(x) or not isinstance(x, str) else x\n",
        "                )\n",
        "                print(f\"✅ **Loaded combined incremental data: {len(df)} rows**\")\n",
        "            except Exception as e:\n",
        "                print(f\"❌ **Error loading combined incremental file: {e}**\")\n",
        "                raise\n",
        "\n",
        "        print(f\"💾 **Saving final annotated data to '{self.annotated_path}'...**\")\n",
        "        try:\n",
        "            df.to_csv(self.annotated_path, index=False)\n",
        "            print(f\"✅ **Successfully saved final annotated data: {len(df)} rows**\")\n",
        "        except Exception as e:\n",
        "            print(f\"❌ **Error saving final annotated file: {e}**\")\n",
        "            raise\n",
        "\n",
        "        if os.path.exists(self.incremental_annotated_path):\n",
        "            try:\n",
        "                os.remove(self.incremental_annotated_path)\n",
        "                print(f\"✅ **Removed temporary incremental file: {self.incremental_annotated_path}**\")\n",
        "            except Exception as e:\n",
        "                print(f\"⚠️ **Failed to remove incremental file: {e}. Manual removal recommended.**\")\n",
        "\n",
        "        self._save_embeddings(df)\n",
        "        return df\n",
        "\n",
        "    def _save_embeddings(self, df):\n",
        "        \"\"\"\n",
        "        Load or generate and save embeddings, and initialize HNSW index.\n",
        "        \"\"\"\n",
        "        print(\"💾 **Generating and saving embeddings**...\")\n",
        "        start_time = time.time()\n",
        "\n",
        "        def format_aspects_sentiments(row):\n",
        "            try:\n",
        "                if pd.isna(row['aspects_sentiments']) or not isinstance(row['aspects_sentiments'], str):\n",
        "                    return \"\"\n",
        "                aspects_dict = json.loads(row['aspects_sentiments'])\n",
        "                return \" \".join([f\"{aspect} {info['sentiment']}\" for aspect, info in aspects_dict.items() if info['confidence'] > 0.6])\n",
        "            except json.JSONDecodeError as e:\n",
        "                print(f\"⚠️ Invalid JSON in aspects_sentiments for row: {e}. Returning empty string.\")\n",
        "                return \"\"\n",
        "            except Exception as e:\n",
        "                print(f\"⚠️ Error processing aspects_sentiments: {e}. Returning empty string.\")\n",
        "                return \"\"\n",
        "\n",
        "        df['enriched_item_description'] = df['itemName'].astype(str) + \" \" + \\\n",
        "                                         df['category'].astype(str) + \" \" + \\\n",
        "                                         df['description'].astype(str) + \" \" + \\\n",
        "                                         df['feature'].astype(str) + \" \" + \\\n",
        "                                         df.apply(format_aspects_sentiments, axis=1)\n",
        "\n",
        "        unique_items_df = df.drop_duplicates(subset='item_unique_id')\n",
        "        unique_item_ids = unique_items_df['item_unique_id'].tolist()\n",
        "        unique_enriched_descriptions = unique_items_df['enriched_item_description'].tolist()\n",
        "\n",
        "        if not os.path.exists(self.enriched_item_descriptions_emb_path):\n",
        "            print(\"🔄 Generating Enriched Item Description Embeddings...\")\n",
        "            enriched_description_embeddings = self.sbert.encode(unique_enriched_descriptions, convert_to_tensor=True, show_progress_bar=True)\n",
        "            np.save(self.enriched_item_descriptions_emb_path, enriched_description_embeddings.cpu().numpy())\n",
        "            print(f\"✅ Saved to {self.enriched_item_descriptions_emb_path}\")\n",
        "            self.enriched_description_embeddings = enriched_description_embeddings.cpu().numpy()\n",
        "        else:\n",
        "            print(\"✅ Loading existing Enriched Item Description Embeddings...\")\n",
        "            self.enriched_description_embeddings = np.load(self.enriched_item_descriptions_emb_path)\n",
        "\n",
        "        self.item_unique_id_to_embedding_map = {\n",
        "            item_id: self.enriched_description_embeddings[i]\n",
        "            for i, item_id in enumerate(unique_item_ids)\n",
        "        }\n",
        "        self.item_unique_id_to_name_map = dict(zip(unique_items_df['item_unique_id'], unique_items_df['itemName']))\n",
        "        self.item_unique_id_to_enriched_text_map = dict(zip(unique_items_df['item_unique_id'], unique_items_df['enriched_item_description']))\n",
        "\n",
        "        # Initialize HNSW index\n",
        "        dim = self.enriched_description_embeddings.shape[1]\n",
        "        self.hnsw_index = hnswlib.Index(space='cosine', dim=dim)\n",
        "        if os.path.exists(self.hnsw_index_path):\n",
        "            print(f\"✅ Loading existing HNSW index from {self.hnsw_index_path}\")\n",
        "            self.hnsw_index.load_index(self.hnsw_index_path, max_elements=len(unique_item_ids))\n",
        "        else:\n",
        "            print(\"🔄 Building HNSW index...\")\n",
        "            self.hnsw_index.init_index(max_elements=len(unique_item_ids), ef_construction=200, M=16)\n",
        "            self.hnsw_index.add_items(self.enriched_description_embeddings, list(range(len(unique_item_ids))))\n",
        "            self.hnsw_index.save_index(self.hnsw_index_path)\n",
        "            print(f\"✅ Saved HNSW index to {self.hnsw_index_path}\")\n",
        "\n",
        "        self.hnsw_index.set_ef(50)  # Set ef for querying\n",
        "        print(f\"✅ Embeddings and HNSW index ready in {time.time() - start_time:.2f} seconds.\")\n",
        "\n",
        "    def _infer_product_category_and_attributes(self, user_query):\n",
        "        \"\"\"\n",
        "        Infer product category and aspects from user query using HNSW for similarity search.\n",
        "        \"\"\"\n",
        "        print(f\"\\n🔍 **Inferring from query**: '{user_query}'\")\n",
        "        query_embedding = self.sbert.encode(user_query, convert_to_numpy=True)\n",
        "\n",
        "        query_aspects = self._extract_dynamic_aspects(user_query)\n",
        "        if not query_aspects:\n",
        "            query_aspects = ['general']\n",
        "\n",
        "        aspect_results = []\n",
        "        for aspect in query_aspects:\n",
        "            input_text = f\"[CLS] {user_query} [SEP] {aspect} [SEP]\"\n",
        "            try:\n",
        "                result = self.absa_pipe(input_text)[0]\n",
        "                if result['score'] > 0.6:\n",
        "                    aspect_results.append((aspect, result['label'].capitalize(), result['score']))\n",
        "            except Exception as e:\n",
        "                print(f\"⚠️ Error processing query aspect '{aspect}': {e}\")\n",
        "\n",
        "        all_item_ids = list(self.item_unique_id_to_embedding_map.keys())\n",
        "        labels, distances = self.hnsw_index.knn_query(query_embedding, k=min(5, len(all_item_ids)))\n",
        "        top_results = [(labels[0][i], 1 - distances[0][i]) for i in range(len(labels[0]))]  # Convert distance to similarity\n",
        "\n",
        "        if top_results[0][1] > 0.4:\n",
        "            best_match_id = all_item_ids[top_results[0][0]]\n",
        "            inferred_item_row = self.df[self.df['item_unique_id'] == best_match_id].iloc[0]\n",
        "            inferred_category = inferred_item_row['category']\n",
        "            print(f\"**Inferred Category**: {inferred_category}, Aspects: {aspect_results}\")\n",
        "            return inferred_item_row['itemName'], inferred_category, aspect_results\n",
        "\n",
        "        print(\"**No specific item strongly inferred. Using general search.**\")\n",
        "        return None, \"general\", aspect_results\n",
        "\n",
        "    def recommend(self, user_query, product_id=None, filter_category=None, top_n_results=10):\n",
        "        \"\"\"\n",
        "        Recommend products using HNSW for fast similarity search, returning name, category, image, score, and confidence.\n",
        "        \"\"\"\n",
        "        print(f\"\\n✨ **Starting Recommendation for query**: '{user_query}'\")\n",
        "        all_item_ids = list(self.item_unique_id_to_embedding_map.keys())\n",
        "\n",
        "        _, _, query_aspects = self._infer_product_category_and_attributes(user_query)\n",
        "        query_aspect_dict = {aspect: sentiment for aspect, sentiment, _ in query_aspects}\n",
        "\n",
        "        if product_id:\n",
        "            if product_id not in self.item_unique_id_to_embedding_map:\n",
        "                print(f\"❌ **Error**: Product ID '{product_id}' not found.\")\n",
        "                return []\n",
        "            query_embedding = self.item_unique_id_to_embedding_map[product_id]\n",
        "        else:\n",
        "            query_embedding = self.sbert.encode(user_query, convert_to_numpy=True)\n",
        "\n",
        "        # Perform HNSW search\n",
        "        labels, distances = self.hnsw_index.knn_query(query_embedding, k=min(top_n_results + 1, len(all_item_ids)))\n",
        "        cosine_scores = [1 - dist for dist in distances[0]]  # Convert distance to similarity\n",
        "\n",
        "        aspect_scores = []\n",
        "        for idx, item_idx in enumerate(labels[0]):\n",
        "            item_id = all_item_ids[item_idx]\n",
        "            item_row = self.df[self.df['item_unique_id'] == item_id].iloc[0]\n",
        "            try:\n",
        "                if pd.isna(item_row['aspects_sentiments']) or not isinstance(item_row['aspects_sentiments'], str):\n",
        "                    item_aspects = {}\n",
        "                else:\n",
        "                    item_aspects = json.loads(item_row['aspects_sentiments'])\n",
        "            except json.JSONDecodeError as e:\n",
        "                print(f\"⚠️ Invalid JSON in aspects_sentiments for item {item_id}: {e}. Using empty aspects.\")\n",
        "                item_aspects = {}\n",
        "            except Exception as e:\n",
        "                print(f\"⚠️ Error processing aspects_sentiments for item {item_id}: {e}. Using empty aspects.\")\n",
        "                item_aspects = {}\n",
        "\n",
        "            aspect_match_score = 0\n",
        "            matched_aspects = 0\n",
        "            top_confidence = 0.0  # Track highest confidence for matching aspects\n",
        "            top_aspect = None\n",
        "            for aspect, q_sentiment in query_aspect_dict.items():\n",
        "                if aspect in item_aspects and item_aspects[aspect]['sentiment'] == q_sentiment:\n",
        "                    confidence = item_aspects[aspect]['confidence']\n",
        "                    aspect_match_score += confidence\n",
        "                    matched_aspects += 1\n",
        "                    if confidence > top_confidence:\n",
        "                        top_confidence = confidence\n",
        "                        top_aspect = aspect\n",
        "            if matched_aspects > 0:\n",
        "                aspect_match_score /= matched_aspects\n",
        "            else:\n",
        "                aspect_match_score = 0.5\n",
        "                top_confidence = 0.5  # Default confidence if no matching aspects\n",
        "\n",
        "            combined_score = 0.7 * cosine_scores[idx] + 0.3 * aspect_match_score\n",
        "            aspect_scores.append((combined_score, item_idx, top_confidence))\n",
        "\n",
        "        aspect_scores.sort(reverse=True)\n",
        "        top_results = aspect_scores[:min(top_n_results + 1, len(all_item_ids))]\n",
        "\n",
        "        recommended_products_info = []\n",
        "        for score, idx, confidence in top_results:\n",
        "            item_id = all_item_ids[idx]\n",
        "            if product_id and item_id == product_id:\n",
        "                continue\n",
        "            product_info = self.df[self.df['item_unique_id'] == item_id].iloc[0]\n",
        "            recommended_products_info.append({\n",
        "                'name': product_info['itemName'],\n",
        "                'category': product_info['category'],\n",
        "                'image': product_info['image'],\n",
        "                'score': score,\n",
        "                'confidence': confidence\n",
        "            })\n",
        "            if len(recommended_products_info) == top_n_results:\n",
        "                break\n",
        "\n",
        "        print(f\"✅ **Recommendation complete. Found {len(recommended_products_info)} products**.\")\n",
        "        return recommended_products_info\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    print(\"Initializing Product Recommender with Dynamic ABSA and HNSW...\")\n",
        "    recommender = ProductRecommender(\n",
        "        dataframe_path=\"/content/drive/MyDrive/finalyearResearch/Second_fixed_image_urls.csv\",\n",
        "        absa_batch_size=16,\n",
        "        absa_chunk_size=200\n",
        "    )\n",
        "\n",
        "    print(\"\\n\\n===================================\")\n",
        "    print(\"🚀 Recommender is ready to use.\")\n",
        "    print(\"===================================\\n\")\n",
        "\n",
        "    print(\"\\n--- Test Case 1: Query with dynamic aspects ---\")\n",
        "    user_input_1 = \"sound is not good enough\"\n",
        "    results_1 = recommender.recommend(user_input_1, top_n_results=10)\n",
        "    print(\"\\n✅ Final Recommendations for Test Case 1:\")\n",
        "    for result in results_1:\n",
        "        print(f\"Name: {result['name']}\")\n",
        "        print(f\"Category: {result['category']}\")\n",
        "        print(f\"Image: {result['image']}\")\n",
        "        print(f\"Score: {result['score']:.3f}\")\n",
        "        print(f\"Confidence: {result['confidence']:.3f}\")\n",
        "        print(\"---\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LLIcc3nzYhXW",
        "outputId": "e05e19e2-070a-40a6-af26-ec15a18f2296"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Initializing Product Recommender with Dynamic ABSA and HNSW...\n",
            "📦 **Overall Device**: CPU\n",
            "⚙️ **ABSA Settings**: chunk_size=200, batch_size=16\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-11-1897184846.py:35: DtypeWarning: Columns (9,10) have mixed types. Specify dtype option on import or set low_memory=False.\n",
            "  self.df_original = pd.read_csv(dataframe_path)\n",
            "/usr/local/lib/python3.11/dist-packages/transformers/convert_slow_tokenizer.py:564: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.\n",
            "  warnings.warn(\n",
            "Device set to use cpu\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ **ABSA Model loaded with device_map='auto'**.\n",
            "✅ **ABSA pipeline initialized (managed by Accelerate)**.\n",
            "✅ **SBERT Model loaded on cpu**.\n",
            "✅ **Ensured directory exists for incremental file: /content/drive/MyDrive/finalyearResearch/annoated**\n",
            "📂 **Final annotated file found at '/content/drive/MyDrive/finalyearResearch/Second_fixed_image_urls.csv'. Loading it.**\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-11-1897184846.py:131: DtypeWarning: Columns (9,10) have mixed types. Specify dtype option on import or set low_memory=False.\n",
            "  df = pd.read_csv(self.annotated_path)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ **Data appears complete with dynamic ABSA annotations.**\n",
            "💾 **Generating and saving embeddings**...\n",
            "✅ Loading existing Enriched Item Description Embeddings...\n",
            "✅ Loading existing HNSW index from /content/drive/MyDrive/finalyearResearch/hnsw_index.bin\n",
            "✅ Embeddings and HNSW index ready in 6.94 seconds.\n",
            "\n",
            "\n",
            "===================================\n",
            "🚀 Recommender is ready to use.\n",
            "===================================\n",
            "\n",
            "\n",
            "--- Test Case 1: Query with dynamic aspects ---\n",
            "\n",
            "✨ **Starting Recommendation for query**: 'sound is not good enough'\n",
            "\n",
            "🔍 **Inferring from query**: 'sound is not good enough'\n",
            "**Inferred Category**: Musical_Instruments, Aspects: [('sound', 'Negative', 0.9812643527984619)]\n",
            "✅ **Recommendation complete. Found 10 products**.\n",
            "\n",
            "✅ Final Recommendations for Test Case 1:\n",
            "Name: ION Audio Tailgater (iPA77) | Portable Bluetooth PA Speaker with Mic, AM/FM Radio, and USB Charge Port\n",
            "Category: Musical_Instruments\n",
            "Image: https://images-na.ssl-images-amazon.com/images/I/31N3dpTs36L.jpg\n",
            "Score: 0.486\n",
            "Confidence: 0.500\n",
            "---\n",
            "Name: Joso PS4 Headphones with Mic, Wired Chat Gaming Headset Xbox One 3.5mm Mono Headphones PS4 One Ear Headphones, Online Live Game Earphone Headband with Mic Stereo for Sony PS4 Slim\n",
            "Category: Video_Games\n",
            "Image: https://images-na.ssl-images-amazon.com/images/I/41Hk3vuAPPL.jpg\n",
            "Score: 0.480\n",
            "Confidence: 0.500\n",
            "---\n",
            "Name: Behringer Ultravoice XM1800S Dynamic Cardioid Vocal and Instrument Microphones, Set of 3\n",
            "Category: Musical_Instruments\n",
            "Image: https://images-na.ssl-images-amazon.com/images/I/51LOyF-rVuL.jpg\n",
            "Score: 0.463\n",
            "Confidence: 0.500\n",
            "---\n",
            "Name: PS3 Ear Force PX21 Gaming Headset\n",
            "Category: Video_Games\n",
            "Image: nan\n",
            "Score: 0.462\n",
            "Confidence: 0.500\n",
            "---\n",
            "Name: SADES SA-708 Stereo Gaming Headset with Microphone (White)\n",
            "Category: Video_Games\n",
            "Image: https://images-na.ssl-images-amazon.com/images/I/41lJ4JJs1OL.jpg\n",
            "Score: 0.458\n",
            "Confidence: 0.500\n",
            "---\n",
            "Name: Behringer MicroHD HD400 Ultra-Compact 2-Channel Hum Destroyer\n",
            "Category: Musical_Instruments\n",
            "Image: https://images-na.ssl-images-amazon.com/images/I/310Wo4Y73NL.jpg\n",
            "Score: 0.457\n",
            "Confidence: 0.500\n",
            "---\n",
            "Name: SADES A6 7.1 Virtual Surround Sound Stereo Over-ear PC USB Gaming Headset with Microphone Vibration Volume Control LED Lights(Electroplating Cover), Black-Orange\n",
            "Category: Video_Games\n",
            "Image: https://images-na.ssl-images-amazon.com/images/I/51ZUk5UFRtL.jpg\n",
            "Score: 0.451\n",
            "Confidence: 0.500\n",
            "---\n",
            "Name: Logitech G35 7.1-Channel Surround Sound Gaming Headset\n",
            "Category: Video_Games\n",
            "Image: https://images-na.ssl-images-amazon.com/images/I/41vSKhjM2FL.jpg\n",
            "Score: 0.451\n",
            "Confidence: 0.500\n",
            "---\n",
            "Name: Sades Wired 3.5mm Stereo Universal Gaming Headset with Microphone (SA708 GT) - Black/Blue\n",
            "Category: Video_Games\n",
            "Image: https://images-na.ssl-images-amazon.com/images/I/51GMoUSRQ6L.jpg\n",
            "Score: 0.442\n",
            "Confidence: 0.500\n",
            "---\n",
            "Name: Motorola HK250 Universal Bluetooth Headset - Retail Packaging - Black\n",
            "Category: Cell_Phones_and_Accessories\n",
            "Image: https://images-na.ssl-images-amazon.com/images/I/316YTJbxvAL._SX38_SY50_CR,0,0,38,50_.jpg\n",
            "Score: 0.441\n",
            "Confidence: 0.500\n",
            "---\n"
          ]
        }
      ]
    }
  ]
}